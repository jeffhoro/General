---
title: "HighNote Monetization Case"
author: "Jeff Horowitz"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Background

**Situation**

HighNote is an online community similar to Spotify (and LinkedIn and OkCupid) that allows both free users and premium users to co-exist.

**Complication**

A general challenge in Freemium communities is getting people to pay premium when they can also use the service for free (at the expense of seeing ads say or not getting some features). However, free users are not as profitable as premium users because the fight for ad dollars from brands is intense. HighNote would like to use a data-driven approach to try to get more free users to become premium users. They are willing to offer a two month free trial, but they do not know who to give this promotion to. They have hired you as a consultant to help them design a targeting strategy.

**Key Question** 

Who are the top 1000 most likely free users to convert to premium?

# 1. Load Libraries

Load in all libraries required for processing and model building.

```{r}
library(tidyverse)
library(skimr)
library(readxl)
library(dplyr) 
library(caret) 
library(class) 
library(rpart.plot) 
library(rpart)  
library(glmnet) 
library(NeuralNetTools) 
library(PRROC) 
library(ROCR) 
library(abind)
library(DMwR) 
library(leaps)
library(RANN)
library(Hmisc)
library(fastDummies)
library(ggplot2)
library(DataExplorer)
library(corrplot)
library(GGally)
library(gridExtra)
library(lightgbm)
library(reshape2)
```

# 2. Data Loading and Pre-Processing

Let's begin by loading the data and doing some exploratory data analysis.

```{r}
# Load the data set 
data <- read_csv('HN_data_PostModule.csv',
                 col_types = 'cnfnnnnnnnnnnnfnnnnnnnnnnff')

# Preview the data
glimpse(data)
skim(data)

# Check for missing values
sum(is.na(data))

# Plot the missing values
plot_missing(data, ggtheme = theme_classic())

# As we can see, there are quite a few missing values in data.
# In order to handle this, we will use 'UNK' to fill in the missing values for 
# categorical data.

# Convert male variable
data$male <-  as.character(data$male)
data$male <- data$male %>% replace_na('UNK') 
data$male <-  as.factor(data$male)

# Convert good_country variable
data$good_country <-  as.character(data$good_country)
data$good_country <- data$good_country %>% replace_na('UNK') 
data$good_country <-  as.factor(data$good_country)

# Convert delta1_good_country variable
data$delta1_good_country <-  as.character(data$delta1_good_country)
data$delta1_good_country <- data$delta1_good_country %>% replace_na('UNK') 
data$delta1_good_country <-  as.factor(data$delta1_good_country)

# Recheck the categorical variables
data %>% keep(is.factor) %>% summary()

# As we can see, there are still quite a few missing values in the numeric data.
# In order to handle this, we will create functions to impute the mean/median for
# the missing values.

# Age (needs to be grouped by gender) so this is left out of the functions. 
data <- data %>%
  group_by(male) %>%
  mutate(age = ifelse(is.na(age), mean(age, na.rm = TRUE), age)) %>%
  ungroup()

# Mean imputation
for (x in colnames(data)) {
  if (x %in% c('avg_friend_age','avg_friend_male','tenure','delta1_avg_friend_age','delta1_avg_friend_male')) {
    data <- data %>%
      mutate(!!x := ifelse(is.na(.data[[x]]),
                           mean(.data[[x]], na.rm = TRUE),
                           .data[[x]]))
  }
}

# Median imputation
for (x in colnames(data)) {
  if (x %in% c('friend_cnt','friend_country_cnt','subscriber_friend_cnt','shouts','delta1_friend_cnt','delta1_friend_country_cnt','delta1_subscriber_friend_cnt','delta1_songsListened','delta1_lovedTracks','delta1_posts','delta1_playlists','delta1_shouts')) {
    data <- data %>%
      mutate(!!x := ifelse(is.na(.data[[x]]),
                           median(.data[[x]], na.rm = TRUE),
                           .data[[x]]))
  }
}

# Recheck the data to ensure no NAs
sum(is.na(data))

# Summary statistics for numerical columns only
numeric_vars <- data %>% select(where(is.numeric))
summary(numeric_vars)

# Summary statistics for categorical columns only
categorical_vars <- data %>% select(where(is.factor))
summary(categorical_vars)

# Correlation matrix for numeric variables
correlation_matrix <- cor(numeric_vars)
melted_corr <- melt(correlation_matrix)
ggplot(data = melted_corr, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name="Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  coord_fixed() + 
  labs(x=NULL, y=NULL)

# Check class imbalance for the target variable
ggplot(data, aes(x = adopter)) +
  geom_bar(fill = "blue", color = "black") +
  labs(title = "Class Distribution", x = "Class", y = "Count") +
  theme_classic()

# Let's normalize the data before proceeding
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

for (x in colnames(data)) {
  if (is.numeric(data[[x]])) {
    data[[x]] <- normalize(data[[x]])
  }
}

```

For the final data pre-processing we will one-hot encode the data such that the categorical variables are transformed into numeric ones. In theory we should drop one of the dummy variables for linear models (i.e., linear or logistic regression), because of potential multicollinearity or the dummy trap. However, during initial testing, model performance was the same for the logistic regression with n-1 and n dummies. Since the other model families (kNN, CART, xgBoost, etc.) can handle n dummies (versus n-1 for linear models), we are going to leave all dummies for further processing.

```{r}
# Create dummies
data <- dummy_cols(data, select_columns = c('male','good_country','delta1_good_country'), remove_selected_columns = TRUE)

# Create x and y data frames
data_y <- data %>% pull("adopter") %>% as.factor()

# Exclude net_user and adopter from the training data
data_x <- data %>% select(-c("net_user", "adopter"))
```

# 3. Training/Testing Split

```{r}
# Split the data into 75% training and 25% testing
set.seed(16)

# Randomly select rows for training and testing
sample_size <- floor(0.75 * nrow(data_x))
train_ind <- sample(seq_len(nrow(data_x)), size = sample_size)

# Create data frames for training, validation, and testing
train_df_x <- data_x[train_ind,]
test_df_x <- data_x[-train_ind,]
train_df_y <- data_y[train_ind]
test_df_y <- data_y[-train_ind]

# Convert the target variable from (0,1) to (X0,X1) as some models need this 
# in order to calculate class probabilities (cannot deal with 0,1 range).
train_df_y <- as.factor(make.names(train_df_y))

# Create an empty data frame to store results from different models
clf_results <- data.frame(matrix(ncol = 5, nrow = 0))
names(clf_results) <- c("Model", "Accuracy", "Precision", "Recall", "F1")
```

# 4. Model Building

## Load Pre-Trained Models

For the purposes of this assignment, all models have been pre-trained. Models have been saved and can be loaded so that we do not have to re-train each model every time the file is run. If the models are not stored in the local directory, you will have to train them again. Loading the models versus training them each time the file is run saves processing time as some models can take hours to train. All training code has been commented out for this assignment to decrease run time.

```{r}
# Load in all models
glm_fit_no_smote <- readRDS('GLM_no_smote.rds')
glm_fit <- readRDS('GLM.rds')
glm_delta_fit <- readRDS('GLM_delta.rds')
dtree_fit <- readRDS('DTREE.rds')
dtree_delta_fit <- readRDS('DTREE_delta.rds')
knn_fit <- readRDS('KNN.rds')
knn_delta_fit <- readRDS('KNN_delta.rds')
rf_fit <- readRDS('RF.rds')
rf_delta_fit <- readRDS('RF_delta.rds')
xg_fit <- readRDS('XG.rds')
xg_delta_fit <- readRDS('XG_delta.rds')
lgb_fit <- readRDS('LGB.rds')
nn_fit <- readRDS('NN.rds')
nn_delta_fit <- readRDS('NN_delta.rds')
lgb_fit_final <- readRDS('LGB_final.rds')
```

## Determine General Model Performance

First, we will define the global training control to be used for all models. Because we have not used the argument 'search' in the trainControl() command, R will default to grid search for each model, producing a more robust model than random search will. Further, we are performing repeated cross-validation with 10 folds, repeated 3 times for added robustness. Lastly, when training each model I am setting metric = 'Kappa' because this can help improve model quality when there are a low percentage of samples in the target class (which was observed during EDA).

Next we will fit a simple logistic regression to determine general model performance on the data.

```{r, results = 'hide'}
# Define the global training control used for all base model training.
fit_control <- trainControl(
  method = 'repeatedcv',
  number = 10,
  repeats = 3,
  allowParallel = TRUE,
  verboseIter = TRUE,
  classProbs = TRUE)

# Let's train a logistic regression to get a sense of general model performance.
# set.seed(16)
# glm_fit_no_smote <- train(train_df_x,
#                  train_df_y,
#                  method = 'glm',
#                  family = 'binomial',
#                  trControl = fit_control,
#                  metric = 'Kappa')
# 
# saveRDS(glm_fit_no_smote, file = 'GLM_no_smote.rds')
```

```{r}
# Evaluate the results
glm_predict_no_smote <- predict(glm_fit_no_smote, test_df_x, type="prob")
y_pred_num <- ifelse(glm_predict_no_smote[,2] > 0.5, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "GLM_no_smote", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))
# Print F1 score
cat("Logisitc Regression (before SMOTE) F1 is ", round (y[["F1"]],3))
```

## Data Rebalancing using SMOTE

As observed in the testing of the logistic regression, the F1 score is extremely low. This is likely due in part to the imbalanced data set with regard to our target variable. During initial EDA, we could see that the target variable was heavily imbalanced but I wanted to test a model before immediately re-balancing the data. With that said, let's now re-balance the data using SMOTE.

```{r}
# Check the distribution of the outcome
round(prop.table(table(select(data, adopter), exclude = NULL)), 4) * 100
round(prop.table(table(train_df_y)), 4) * 100
round(prop.table(table(test_df_y)), 4) * 100

# Combine training sets into single set for balancing
set.seed(16)
combined_training <- cbind(train_df_x, train_df_y)
SMOTE_training_balanced <- SMOTE(train_df_y ~ .,
                                 combined_training,
                                 perc.over = 100,
                                 perc.under = 200)

# Recheck the distribution of the outcome.
# The training data only should be re-balanced. 
round(prop.table(table(select(data, adopter), exclude = NULL)), 4) * 100
round(prop.table(table(SMOTE_training_balanced$train_df_y)), 4) * 100
round(prop.table(table(test_df_y)), 4) * 100

# Split into x and y data sets
train_df_x <- SMOTE_training_balanced %>% select(-train_df_y)
train_df_y <- SMOTE_training_balanced %>% pull(train_df_y) %>% as.factor()
```

As we can see, the training data is now balanced with regard to our target variable. Let's proceed with testing a variety of different models to assess performance. For each model, we will fit two models, one using only the previous time period's variables (as denoted by 'delta1' in the variable name), and one using all of the variables. As such let's break out the previous time period's variables and store those separately for model training.

```{r}
# Split the previous time period's variables out from the total variable set so that we can train a model on each variable set.
train_df_delta_x <- train_df_x %>% select(-c("friend_cnt",
                                             'avg_friend_age',
                                             'avg_friend_male',
                                             'friend_country_cnt',
                                             'subscriber_friend_cnt',
                                             'songsListened',
                                             'lovedTracks',
                                             'posts',
                                             'playlists',
                                             'shouts',
                                             'good_country_0',
                                             'good_country_1',
                                             'good_country_UNK'))
```

## Logistic Regression

### Delta Only Model

```{r}
# Delta Only
# set.seed(16)
# glm_delta_fit <- train(train_df_delta_x,
#                       train_df_y,
#                       method = "glm",
#                       family = "binomial",
#                       trControl = fit_control,
#                       metric = 'Kappa')
# 
# saveRDS(glm_delta_fit, file = 'GLM_delta.rds')

# Evaluate the results
glm_delta_predict <- predict(glm_delta_fit, test_df_x, type="prob")
y_pred_num <- ifelse(glm_delta_predict[,2] > 0.6, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "GLM_delta", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("Logistic Regression with delta variables F1 is ", round (y[["F1"]],3))
```

### Full Model

```{r}
# All Variables
# set.seed(16)
# glm_fit <- train(train_df_x,
#                  train_df_y,
#                  method = "glm",
#                  family = "binomial",
#                  trControl = fit_control,
#                  metric = 'Kappa')
#
# saveRDS(glm_fit, file = 'GLM.rds')

# Evaluate the results
glm_predict <- predict(glm_fit, test_df_x, type="prob")
y_pred_num <- ifelse(glm_predict[,2] > 0.6, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "GLM", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("Logistic Regression with all variables F1 is ", round (y[["F1"]],3))
```

## Decision Trees

### Delta Only Model

```{r}
# Delta Only
# set.seed(16)
# dtree_delta_fit <- train(train_df_delta_x,
#                    train_df_y,
#                    method = "rpart",
#                    trControl = fit_control,
#                    metric = 'Kappa')
# 
# saveRDS(dtree_delta_fit, file = 'DTREE_delta.rds')

# Evaluate the results
dtree_delta_predict <- predict(dtree_delta_fit, test_df_x, type="prob")
y_pred_num <- ifelse(dtree_delta_predict[,2] > 0.6, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "DTREE_delta", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("Decision Tree with delta variables F1 is ", round (y[["F1"]],3))
```

### Full Model

```{r}
# All Variables
# set.seed(16)
# dtree_fit <- train(train_df_x,
#                    train_df_y,
#                    method = "rpart",
#                    trControl = fit_control,
#                    metric = 'Kappa')
# 
# saveRDS(dtree_fit, file = 'DTREE.rds')

# Evaluate the results
dtree_predict <- predict(dtree_fit, test_df_x, type="prob")
y_pred_num <- ifelse(dtree_predict[,2] > 0.7, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results dataframe
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "DTREE", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("Decision Tree with all variables F1 is ", round (y[["F1"]],3))
```

## K-Nearest Neighbors

### Delta Only Model

```{r}
# Delta Only
# set.seed(16)
# knn_delta_fit <- train(train_df_delta_x,
#                        train_df_y,
#                        method = "knn",
#                        trControl = fit_control,
#                        metric = 'Kappa')
# 
# saveRDS(knn_delta_fit, file = 'KNN_delta.rds')

# Evaluate the results
knn_delta_predict <- predict(knn_delta_fit, test_df_x, type="prob")
y_pred_num <- ifelse(knn_delta_predict[,2] > 0.5, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "KNN_delta", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("KNN with delta variables F1 is ", round (y[["F1"]],3))
```

### Full Model

```{r}
# All Variables
# set.seed(16)
# knn_fit <- train(train_df_x,
#                  train_df_y,
#                  method = "knn",
#                  trControl = fit_control,
#                  metric = 'Kappa')
#
# saveRDS(knn_fit, file = 'KNN.rds')

# Evaluate the results
knn_predict <- predict(knn_fit, test_df_x, type="prob")
y_pred_num <- ifelse(knn_predict[,2] > 0.5, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "KNN", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("KNN with all variables F1 is ", round (y[["F1"]],3))
```

## Random Forest

### Delta Only Model

```{r}
# Delta Only
# set.seed(16)
# rf_delta_fit <- train(train_df_delta_x,
#                   train_df_y,
#                   method = "ranger",
#                   trControl = fit_control,
#                   metric = 'Kappa')
# 
# saveRDS(rf_delta_fit, file = 'RF_delta.rds')

# Evaluate the results
rf_delta_predict <- predict(rf_delta_fit, test_df_x, type = 'prob')
y_pred_num <- ifelse(rf_delta_predict[,2] > 0.6, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "RF_delta", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("Random Forest with delta variables F1 is ", round (y[["F1"]],3))
```

### Full Model

```{r}
# All Variables
# set.seed(16)
# rf_fit <- train(train_df_x,
#                 train_df_y,
#                 method = "ranger",
#                 trControl = fit_control,
#                 metric = 'Kappa')
#
# saveRDS(rf_fit, file = 'RF.rds')

# Evaluate the results
rf_predict <- predict(rf_fit, test_df_x, type = 'prob')
y_pred_num <- ifelse(rf_predict[,2] > 0.6, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "RF", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("Random Forest with all variables F1 is ", round (y[["F1"]],3))
```

## XGBoost

### Delta Only Model

```{r}
# Delta Only
# set.seed(16)
# xg_delta_fit <- train(train_df_delta_x,
#                       train_df_y,
#                       method = 'xgbTree',
#                       trControl = fit_control,
#                       metric = 'Kappa')
# 
# saveRDS(xg_delta_fit, file = 'XG_delta.rds')

# Evaluate the results
xg_delta_predict <- predict(xg_delta_fit, test_df_x, type = "prob")
y_pred_num <- ifelse(xg_delta_predict[,2] > 0.6, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "XG_delta", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("XGBoost with delta variables F1 is ", round (y[["F1"]],3))
```

### Full Model

```{r}
# All Variables
# set.seed(16)
# xg_fit <- train(train_df_x,
#                 train_df_y,
#                 method = 'xgbTree',
#                 trControl = fit_control,
#                 metric = 'Kappa')
#
# saveRDS(xg_fit, file = 'XG.rds')

# Evaluate the results
xg_predict <- predict(xg_fit, test_df_x, type = "prob")
y_pred_num <- ifelse(xg_predict[,2] > 0.6, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "XG", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("XGBoost with all variables F1 is ", round (y[["F1"]],3))
```

## LightGBM

### Full Model

```{r}
# Change the levels of the training y data back to (0,1) for LightGBM model
train_df_y_lgb <- train_df_y
levels(train_df_y_lgb) <- c(0,1)

# Define matrices for model
train_matrix <- as.matrix(train_df_x)
train_labels <- as.matrix(train_df_y_lgb)

test_matrix <- as.matrix(test_df_x)
test_labels <- as.matrix(test_df_y)

# Create LightGBM dataset objects
dtrain <- lgb.Dataset(data = train_matrix, label = train_labels, free_raw_data = FALSE)
dtest <- lgb.Dataset(data = test_matrix, label = test_labels, free_raw_data = FALSE)

# Train the model
# lgb_fit <- lgb.train(
#   data = dtrain,
#   nrounds = 1000,
#   valids = list(test = dtest),
#   early_stopping_rounds = 10,
#   verbose = 1
# )
# 
# saveRDS(lgb_fit, file = 'LGB.rds')

# Evaluate the results
lgb_predict <- predict(lgb_fit, test_matrix, type = "prob")
y_pred_num <- ifelse(lgb_predict > 0.6, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_labels), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_labels), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_labels), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "LGB", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))
# Print F1 score
cat("LightGBM with all variables F1 is ", round (y[["F1"]],3))
```

## Neural Network

### Delta Only Model

```{r}
# Delta Only
# set.seed(16)
# nn_delta_fit <- train(train_df_delta_x,
#                        train_df_y,
#                        method = 'nnet',
#                        trace = F,
#                        trControl = fit_control,
#                        linout = 0,
#                        stepmax = 100,
#                        threshold = 0.01,
#                        metric = 'Kappa')
# 
# saveRDS(nn_delta_fit, file = 'NN_delta.rds')

# Evaluate the results
nn_delta_predict <- predict(nn_delta_fit, test_df_x, type = 'prob')
y_pred_num <- ifelse(nn_delta_predict[,2] > 0.6, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = 'prec_recall')

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "NN_delta", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("Neural Network with delta variables F1 is ", round (y[["F1"]],3))
```

### Full Model

```{r}
# All Variables
# set.seed(16)
# nn_fit <- train(train_df_x,
#                 train_df_y,
#                 method = 'nnet',
#                 trace = F,
#                 trControl = fit_control,
#                 linout = 0,
#                 stepmax = 100,
#                 threshold = 0.01,
#                 metric = 'Kappa')
# 
# saveRDS(nn_fit, file = 'NN.rds')

# Evaluate the results
nn_predict <- predict(nn_fit, test_df_x, type = 'prob')
y_pred_num <- ifelse(nn_predict[,2] > 0.7, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_df_y), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "NN", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("Neural Network with all variables F1 is ", round (y[["F1"]],3))
```

# 5. Model Evaluation

Let's first look at the metrics across all the models. This will gives us an indication of which models are performing better than others.

```{r}
# Display the metrics across models
print(clf_results)
```

As we can see, using the full set of variables gives better model performance for all models with the exception of the k-nearest neighbors models. We will drop the k-nearest neighbors and all models built only with the previous time period's variables for further model evaluation.

We can next plot the F1 scores.

```{r}
# Plot F1 Scores
ggplot(clf_results %>%
         filter(Model %in% c('GLM','DTREE','RF','XG','LGB','NN')) %>% 
         arrange(desc(F1)) %>%
         mutate(Model=factor(Model, levels=Model) ), 
       aes(x = Model, y = F1)) +
  geom_bar(stat = "identity" , width=0.3, fill="blue") + 
  coord_cartesian(ylim = c(0.1, 0.4)) +
  geom_hline(aes(yintercept = max(F1)),
             colour = "red",linetype="dashed") +
  ggtitle("F1 Scores") +
  theme(plot.title = element_text(color="black", size=10, hjust = 0.5)) +
  theme_classic()
```

We can continue our model evaluation by plotting the ROC and Lift curves for the top performing models.

## ROC Curves

```{r}
# List of predictions
preds_list <- list(glm_predict[,2],
                   dtree_predict[,2],
                   rf_predict[,2],
                   xg_predict[,2],
                   lgb_predict,
                   nn_predict[,2])

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(test_df_y), m)

# Calculate ROC
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")

# Calculate AUC
AUC_models <- performance(pred, "auc")
auc_LR <-  round(AUC_models@y.values[[1]], 3)
auc_DTREE <- round(AUC_models@y.values[[2]], 3)
auc_RF <- round(AUC_models@y.values[[3]], 3)
auc_XG <- round(AUC_models@y.values[[4]], 3)
auc_LGB <- round(AUC_models@y.values[[5]], 3)
auc_NN <- round(AUC_models@y.values[[6]], 3)

# Create new data framce for final models
clf_results_final <- clf_results %>% filter(Model %in% c('GLM','DTREE','RF','XG','LGB','NN'))

# Add AUC values to results data frame
clf_results_final$AUC <- c(auc_LR,
                     auc_DTREE,
                     auc_RF,
                     auc_XG,
                     auc_LGB,
                     auc_NN)

# Plot the ROC curves
plot(rocs, col = as.list(1:m), main = "ROC Curves")
abline(a = 0, b = 1, lty = 2, col = 'red')
legend(x = "bottomright",
       legend = c(paste("LR - ", auc_LR),
                  paste("DTREE - ", auc_DTREE),
                  paste("RF - ", auc_RF),
                  paste("XG - ", auc_XG),
                  paste("LGB - ", auc_LGB),
                  paste("NN - ", auc_NN)),
       fill = 1:m)
```

## Lift Curves

```{r}
lifts <- performance(pred, "lift", "rpp")

# Plot the Lift curves
plot(lifts, col = as.list(1:m), main = "Lift Curves")
legend(x = "topright", 
       legend = c('LR',
                  'DTREE',
                  'RF',
                  'XG',
                  'LGB',
                  'NN')
       , fill = 1:m)
```

# 6. Model Selection

To determine which model to select, we can compare the F1 scores in addition to the AUC values for the top performing models.

```{r}
print(clf_results_final)
```

As we can see, the LightGBM model has the highest F1 and AUC scores across the models selected for evaluation. As such, let's continue to tune the LightGBM model to see if we can increase performance even further.

```{r}
# Define hyper-parameter tuning grid
# params <- list(
#   objective = "binary",
#   metric = "auc",
#   boosting = "gbdt",
#   num_leaves = 31,
#   learning_rate = 0.05,
#   feature_fraction = 0.9,
#   bagging_fraction = 0.9,
#   bagging_freq = 5,
#   min_data_in_leaf = 20,
#   lambda_l1 = 0.9,
#   lambda_l2 = 0
# )

# Train the model
# lgb_fit_final <- lgb.train(
#   params = params,
#   data = dtrain,
#   nrounds = 1000,
#   valids = list(test = dtest),
#   early_stopping_rounds = 10,
#   verbose = 1
# )
# 
# saveRDS(lgb_fit_final, file = 'LGB_final.rds')

# Evaluate the results
lgb_final_predict <- predict(lgb_fit_final, test_matrix, type = "prob")
y_pred_num <- ifelse(lgb_final_predict > 0.6, 1, 0)
confusionMatrix(as.factor(y_pred_num), as.factor(test_labels), positive = "1", mode = "prec_recall")

# Add results into clf_results data frame
x <- confusionMatrix(as.factor(y_pred_num), as.factor(test_labels), positive = "1", mode = "prec_recall")[["overall"]]
y <- confusionMatrix(as.factor(y_pred_num), as.factor(test_labels), positive = "1", mode = "prec_recall")[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "LGB_final", 
                                             Accuracy = round (x[["Accuracy"]],3), 
                                             Precision = round (y[["Precision"]],3), 
                                             Recall = round (y[["Recall"]],3), 
                                             F1 = round (y[["F1"]],3))

# Print F1 score
cat("LightGBM tuned with all variables F1 is ", round (y[["F1"]],3))
```

We can calculate the AUC of the tuned LightGBM model and display the results of hyper-parameter tuning compared to the original LightGBM model.

```{r}
# Calculate the AUC for the final LightGBM model and add to data frame to compare to original LightGBM model.
preds_list <- list(lgb_final_predict)

# List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(test_df_y), m)

# Calculate ROC
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")

# Calculate AUC
AUC_models <- performance(pred, "auc")
auc_LGB_final <-  round(AUC_models@y.values[[1]], 3)

# Create new data framce for final LightGBM models
clf_results_lgb <- clf_results %>% filter(Model %in% c('LGB','LGB_final'))

# Add AUC values to LightGBM data frame
clf_results_lgb$AUC <- c(auc_LGB,
                         auc_LGB_final)

print(clf_results_lgb)
```

Tuning the model increased performance! We will use this final LightGBM model for our final prediction.

# 7. Prediction

We will now use the tuned LightGBM model to predict the top 1000 currently free users who are most likely to convert to premium and list their probability of conversion. We will use the entire data set filtered on currently free users (adopter = 0) for the prediction. 

As best practice, we should perform the same data pre-processing on any new data that we will use for prediction. However, because we are using the full data set and have already performed the pre-processing on this data, we can reassign the pre-processed data to a new data frame and use this new data frame to make our final predictions.

```{r}
# Filter for free users
target_list <- data %>% filter(adopter == 0)

# Remove net_user and adopter fields
target_list_x <- target_list %>% select(-c('net_user','adopter'))

# Create matrix for LightGBM model
target_matrix <- as.matrix(target_list_x)

# Predict on new data
lgb_target_predict <- predict(lgb_fit_final, target_matrix, type = "prob")

# Add new field to the target list data frame
target_list$ProbConvert <- lgb_target_predict

# Arrange the target list by ProbConvert descending
target_list <- target_list %>% arrange(desc(ProbConvert))

successConvert <- target_list[1:1000,] %>% filter(ProbConvert > .85) %>% tally()

# Save target list to csv
write.csv(target_list, file = "targetList.csv", row.names = TRUE)

cat('If we target the top 1000 currently free users', successConvert[[1]], 'out of 1000 have a chance greater than 85% of converting.')

```

# 8. Conclusion

As we can see, after targeting the top 1000 currently free users, 701 out of the 1000 have a greater than 85% chance of converting and all of them have a greater than 80% chance of converting. Our model seems to be performing pretty well despite the low F1 scores observed. The full scored list is available in the accompanying file "targetList.csv". It is sorted by the ProbConvert column descending. The top 1000 users can be identified as the top 1000 users in the file. 

To expand further, we could hand over this target list to the marketing department at HighNote and have them target the top 1000 users on our list. In order to make sure that we can compute the correct ROI if we target these users, we can use the expected value framework in conjunction with A/B testing. At a high-level this means calculating the expected value of each conversion of a free user and testing to make sure that any increase in conversion rates is due to our targeting strategy. Next, I will outline a detailed implementation of this strategy.

**1. Calculate Expected Value of Conversion**

Each user in the target group has an associated probability of converting (ProbConvert field) from our model. The expected value of conversion for each user can be calculated by taking the probability of conversion and multiplying it by the revenue increase (difference in revenue between a free and premium user). If we sum up the expected value for each of the target users, we arrive at our total expected revenue.
$$
Expected\;Value\;(EV) = P(conversion)\;\times\;Revenue\;Increase
$$

**2. Run A/B Testing**

Now that we have an expected revenue, we need to test our strategy and one of the most effective ways we can do this is through A/B testing. For A/B testing we need a test and control group. The test group is the 1000 users we identified by using our model and will receive the two-month free trial promotion. For the control group, we can simply select at random another 1000 users and do not give them the two-month free trial promotion. After two months (the length of the free trial), we can measure the actual conversion rates of the test and control groups, and calculate the uplift, which compares the conversion rate of the test group to that of the control group. 

**3. Calculate the ROI**

In order to calculate our ROI, we need the following:<br>

 - **Total Revenue from Conversions**: calculated as the total amount generated from users who converted to premium in the test group.<br>
 
 - **Cost of Campaign**: calculated as the total cost of offering the free trial, which is considered an opportunity cost, plus any marketing or administrative costs. We can calculate this per user and multiply by 1000 to get the total cost of the campaign.

$$
Total\;Revenue\;from\;Conversions = Actual\;Conversions\;\times\;Revenue\;per\;Premium\;User
$$

$$ 
Cost\;of\;Campaign = (Monthly\;Premium\;Revenue\times2)\times1000
$$
$$
ROI = \frac{Total\;Revenue\;from\;Conversions\;-\;Cost\;of\;Campaign}{Cost\;of\;Campaign\;}\;\times\;100
$$

On a final note, if we had the lifetime value (LTV) of a premium user, we could calculate a more accurate ROI because LTV considers the average duration a user stays premium versus just the immediate revenue from initial conversion.

$$
LTV = Average\;Duration\;for\;Premium\;User\;\times\;Revenue\;per\;Subscription\;Period
$$

We could then adjust our revenue and ROI calculations based on the expected LTV of each free user converted to premium.

**4. Deployment and Monitoring**

Once the A/B testing is complete and the ROI is calculated, we can deploy the model (assuming it has met the standards of the marketing department), track the actual conversions, and continue to improve our model. After observing how accurate our model was in predicting the conversion rates, we can refine the model for future campaigns, incorporating additional features into the model and/or adjusting thresholds based on the goals of these new campaigns.
